# Use ReLU for activation in hidden layers
# Use Softmax for activation in output layer
# Use cross entropy for loss